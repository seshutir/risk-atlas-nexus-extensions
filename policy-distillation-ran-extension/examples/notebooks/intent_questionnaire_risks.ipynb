{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "- Generate data as a set of intents\n",
    "- Run through the pipeline to get a set of (initial) risk prioritizations\n",
    "- Create LLM-as-a-Judge to reprioritize each risk\n",
    "- For each risk run the policy distillation -> obtain a policy for when this risk should be reprioritized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.components.llms.rits_component import RITSComponent\n",
    "from dotenv import load_dotenv\n",
    "from langextract.resolver import ResolverParsingError\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import ast\n",
    "import math\n",
    "import logging\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import langextract as lx\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging\n",
    "logger = logging.getLogger('logger')\n",
    "logger.setLevel(logging.INFO)\n",
    "fh = logging.FileHandler(f'logs/{datetime.datetime.now().strftime(\"%m_%d__%H_%M\")}.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_type_timeout(lx.inference.OllamaLanguageModel):\n",
    "    def _ollama_query(self,*args,**kwargs):\n",
    "        kwargs.setdefault('timeout',120)\n",
    "        return super()._ollama_query(*args,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a synthetic dataset of intents\n",
    "# Use an LLM to generate a set of intents as scenarios where AI can be used\n",
    "def generate_intent_data(llm_component, n_samples=10):\n",
    "    context = \"\"\"\n",
    "                You are a helpful AI assistant that can generate diverse AI usecase scenarios.\n",
    "              \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "                Generate a list of {n_samples} scenarios in which AI could be used to automate or replaces specific tasks. \n",
    "                Include information about how this tool will be used or monitored.\n",
    "\n",
    "                Here are some examples of scenarios:\n",
    "                    - AI medical chatbot for internal use. Ouptuts monitored by medical professionals. Not used for diagnostics or patient care.\n",
    "                    - AI-enhanced interior design tool that helps users design their homes. Used by people decorating their homes. Verified for hallucinatory behavior.\n",
    "                    - Storytelling app for kids where you can choose characters and different plot features and AI generates a story. Guardrails for toxic and harmful ouptut are implemented.\n",
    "\n",
    "                Return ONLY the list of scenarios formatted as a Python list\n",
    "                Do not include any additional information or explanations.\n",
    "            \"\"\" \n",
    "\n",
    "    intents = llm_component.send_request(context, prompt)\n",
    "    intents = ast.literal_eval(intents)\n",
    "\n",
    "    print(len(intents))\n",
    "    \n",
    "    return intents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts structured information about the intent such as AI tasks, domain, stakeholders\n",
    "def extract_intent_features(input_text):\n",
    "    # 1. Define the prompt and extraction rules\n",
    "    prompt = \"\"\"\n",
    "        Get the answers only to the following questions. Make sure to provide the mentioned attributes for all questions.\n",
    "        Classify the confidence attribute as \"Direct answer from the input text\" or \"Likely answer from the  input text\" or \"Unable to answer from input text\". \n",
    "    \n",
    "        1. \"What domain does your use request fall under amongst the following options?\n",
    "        Customer service/support, Technical, Information retrieval, Strategy, Code/software engineering, Communications, IT/business automation, Writing assistant, Financial,\n",
    "        Talent and Organization including HR, Product, Marketing, Cybersecurity, Healthcare, User Research, Sales, Risk and Compliance, Design, Other\", attribute: domain. \n",
    "        There must be a direct or likely answer for this question. \n",
    "        2. In which environment is the system used?, attribute: environment\n",
    "        3. Is there private data for this use-case?, attribute: private data\n",
    "        4. What techniques are utilised in the system? Multi-modal: {Document Question/Answering, Image and text to text, Video and text to text, visual question answering}, \n",
    "                                                       Natural language processing: {feature extraction, fill mask, question answering, sentence similarity, summarization, table question answering, text classification, text generation, token classification, translation, zero shot classification},\n",
    "                                                       computer vision: {image classification, image segmentation, text to image, object detection}, audio:{audio classification, audio to audio, text to speech}, tabular: {tabular classification, tabular regression}, \n",
    "                                                       reinforcement learning                     \n",
    "            attribute: ai_techniques. There must be a direct or likely answer for this question. \n",
    "        5. Who is the subject as per the intent?\n",
    "        \"\"\"\n",
    "    \n",
    "    # 2. Provide a high-quality example to guide the model\n",
    "    examples = [\n",
    "        lx.data.ExampleData(\n",
    "            text=\"Generate personalized, relevant responses, recommendations, and summaries of claims for customers to support agents to enhance their interactions with customers. LLM has been robustly tested and shows high accuracy with no bias or fairness issues. There are guardrais for privacy and security. The main subjects are customers/claimants (whose data and preferences the LLM uses), support agents (who receive the personalized content), and the organization’s stakeholders (compliance, security, and tech teams) who oversee the system’s integrity\",\n",
    "            extractions=[\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"What techniques are utilised in the system from the given options? Multi-modal: {Document Question/Answering, Image and text to text, Video and text to text, visual question answering}, Natural language processing: {feature extraction, fill mask, question answering, sentence similarity, summarization, table question answering, text classification, text generation, token classification, translation, zero shot classification}, computer vision: {image classification, image segmentation, text to image, object detection}, audio:{audio classification, audio to audio, text to speech}, tabular: {tabular classification, tabular regression}, reinforcement learning\",\n",
    "                    extraction_text=\"1. Generate personalized, relevant responses 2. summaries of claims \",\n",
    "                    attributes={\"confidence\": \"Likely answer from the  input text\", \"ai_techniques\": \"Natural language processing: {Text classification, sentence similarity, question answering, summarization, text generation} \"}\n",
    "                ),\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"What domain does your use request fall under? Customer service/support, Technical, Information retrieval, Strategy, Code/software engineering, Communications, IT/business automation, Writing assistant, Financial, Talent and Organization including HR, Product, Marketing, Cybersecurity, Healthcare, User Research, Sales, Risk and Compliance, Design, Other\",\n",
    "                    extraction_text=\"summaries of claims for customers to support agents to enhance their interactions with customers\",\n",
    "                    attributes={\"confidence\": \"Directly from the  input text\", \"domain\": \"Customer service/support\"}\n",
    "                ),\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"In which environment is the system used?\",\n",
    "                    extraction_text=\" \",\n",
    "                    attributes={\"confidence\": \"Likely answer from the  input text\", \"environment\": \"digital environment, specifically designed for online interactions between customers and support agents\"}\n",
    "                ),\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"Is there private data for this use-case?\",\n",
    "                    extraction_text=\" \",\n",
    "                    attributes={\"confidence\": \"Unable to answer from  input text\", \"private_data\": \"Unable to answer from the text\" }\n",
    "                ),\n",
    "                lx.data.Extraction(\n",
    "                    extraction_class=\"Who is the subject as per the intent?\",\n",
    "                    extraction_text=\"The main subjects are customers/claimants (whose data and preferences the LLM uses), support agents (who receive the personalized content), and the organization’s stakeholders (compliance, security, and tech teams) who oversee the system’s integrity\",\n",
    "                    attributes={\"confidence\": \"Likely answer from the  input text\", \"subject_data\": \"customers/claimants, support agents, compliance, security, and tech teams\" }\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    \n",
    "    template = \"\"\"Use the following pieces of context to answer the question at the end. Quantify the statement as most likely if it is not in the context. If no answer is possible from the context, state that as it is not possible to give that from the context. Succinct answer and appended to the context \\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    model = OllamaLLM(model=\"llama3:8b\")\n",
    "    chain = prompt | model\n",
    "\n",
    "    techniques = chain.invoke({\"question\":\"What techniques are utilised in the system from the given options? Multi-modal: {Document Question/Answering, Image and text to text, Video and text to text, visual question answering}, Natural language processing: {feature extraction, fill mask, question answering, sentence similarity, summarization, table question answering, text classification, text generation, token classification, translation, zero shot classification}, computer vision: {image classification, image segmentation, text to image, object detection}, audio:{audio classification, audio to audio, text to speech}, tabular: {tabular classification, tabular regression}, reinforcement learning\",\"context\": input_text})\n",
    "    input_text = input_text + techniques\n",
    "    n_tries = 0\n",
    "    while n_tries < 10:\n",
    "        try:\n",
    "            result = lx.extract(\n",
    "                        text_or_documents=input_text,\n",
    "                        prompt_description=prompt,\n",
    "                        examples=examples,\n",
    "                        language_model_type=model_type_timeout, #lx.inference.OllamaLanguageModel,\n",
    "                        model_id=\"llama3:8b\",  # or any Ollama model\n",
    "                        model_url=\"http://localhost:11435\",\n",
    "                        fence_output=False,\n",
    "                        use_schema_constraints=False)\n",
    "\n",
    "            return result\n",
    "        except ResolverParsingError:\n",
    "            n_tries += 1\n",
    "                    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ai_task_names():\n",
    "    with open(\"risk_matrix/ai_tasks_risk_matrix.json\") as f:\n",
    "        tasks = json.load(f)\n",
    "    \n",
    "    ai_tasks = []\n",
    "    \n",
    "    for task in tasks[\"tasks\"]:\n",
    "        ai_tasks.append(task[\"name\"])\n",
    "    \n",
    "    return ai_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_extraction_to_ai_risks(intent_ai_tasks, ai_tasks):\n",
    "    # Extracts AI tasks from the intent text\n",
    "    template = \"\"\"You are a ai task analysis assistant. Given the following freeform text, extract which ai tasks are PRESENT from this list: {ai_tasks} - If an ai task is explicitly mentioned as NOT present (e.g., 'no regression'), exclude it. - If close enough words are used (e.g., 'doc qnA' → 'Document Question Answering'), map them to the correct ai task. - Only return the ai tasks that are clearly PRESENT. Answer only with the ai tasks in list format. DO NOT INCLUDE any preamble or explanations \\n\\n{intent_ai_tasks}\\n:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    model = OllamaLLM(model=\"llama3:8b\")\n",
    "    \n",
    "    chain = prompt | model\n",
    "    \n",
    "    intent_ai_techniques = chain.invoke({\"ai_tasks\": ai_tasks, \"intent_ai_tasks\": intent_ai_tasks})\n",
    "\n",
    "    intent_ai_techniques = ast.literal_eval(intent_ai_techniques)\n",
    "\n",
    "    ai_task_indices = [ai_tasks.index(x.strip())  for x in intent_ai_techniques if x.strip() in ai_tasks]\n",
    "    ai_task_indices\n",
    "\n",
    "    return ai_task_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_risks(dict_list):\n",
    "    priority = {\"Low\": 1, \"Medium\": 2, \"High\": 3, \"Critical\": 4}\n",
    "    \n",
    "    result = {key: \"Low\" for key in dict_list[0].keys()}\n",
    "    \n",
    "    for d in dict_list:\n",
    "        for key, value in d.items():\n",
    "            if priority[value] > priority[result[key]]:\n",
    "                result[key] = value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_risks(ai_task_indices):\n",
    "    # Loads baseline risks from a json \n",
    "    with open(\"risk_matrix/ai_tasks_risk_matrix.json\") as f:\n",
    "        tasks = json.load(f)\n",
    "    \n",
    "    all_baseline_risks = [tasks[\"tasks\"][index][\"baseline_risks\"] for index in ai_task_indices]\n",
    "    return all_baseline_risks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples=100):\n",
    "    'Generates an intent-risk-baseline priority dataset we can use to reprioritize risks'\n",
    "    try:\n",
    "        df = pd.read_csv('datasets/intents.csv', header=0)\n",
    "    except FileNotFoundError:\n",
    "        data = []\n",
    "        \n",
    "        llm_component = RITSComponent('llama-3-3-70b-instruct', 'meta-llama/llama-3-3-70b-instruct')    \n",
    "        intent_data = generate_intent_data(llm_component, n_samples=n_samples)\n",
    "        ai_tasks_names = get_ai_task_names()\n",
    "    \n",
    "        for intent_text in intent_data:\n",
    "            # extract features of the intent like stakeholders, AI tasks, domains etc\n",
    "            extraction = extract_intent_features(intent_text).extractions[0]\n",
    "\n",
    "            if extraction.attributes is None or \"ai_techniques\" not in extraction.attributes.keys():\n",
    "                continue\n",
    "    \n",
    "            # extract AI tasks present in the scenario\n",
    "            ai_task_indices = map_extraction_to_ai_risks(extraction.attributes[\"ai_techniques\"], ai_tasks_names)\n",
    "            baseline_risks = get_baseline_risks(ai_task_indices)\n",
    "    \n",
    "            final_risk_matrix = consolidate_risks(baseline_risks)\n",
    "    \n",
    "            for risk, priority in final_risk_matrix.items():\n",
    "                data.append((intent_text, risk, priority))\n",
    "\n",
    "        df = pd.DataFrame(data, columns=['intent', 'risk', 'priority'])\n",
    "        df.to_csv('datasets/intents.csv', index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.guardians.judge import Judge\n",
    "from src.utils.rits_util import post_rits_req\n",
    "\n",
    "class RiskReprioritizationJudge(Judge):\n",
    "    ''' Simple LLM-as-a-Judge model that decides whether a risk should be reprioritized'''\n",
    "    def __init__(self, model_name, model_served_name, config):\n",
    "        super().__init__(config)\n",
    "        self.model_name = model_name\n",
    "        self.model_served_name = model_served_name\n",
    "        self.context = \"\"\"\n",
    "                        Given a task, AI risk, and a baseline estimated impact of this risk on the task decide if the impact should be changed.\n",
    "                        Answer with one of the possible risk impacts: Low, Medium, High or Critical. \n",
    "\n",
    "                        Here are some examples:\n",
    "                        \n",
    "                        AI Task: Generate personalized, relevant responses, recommendations, and summaries of claims for customers to support agents to enhance their interactions with customers. \n",
    "                                 LLM has been thoroughly tested and shows high accuracy with no bias or fairness issues. \n",
    "                                 There are guardrais for privacy and security. \n",
    "                                 The main subjects are customers/claimants (whose data and preferences the LLM uses), support agents (who receive the personalized content), \n",
    "                                 and the organization’s stakeholders (compliance, security, and tech teams) who oversee the system’s integrity. \n",
    "                        Risk: Fairness\n",
    "                        Risk impact: Medium\n",
    "                        New impact: Low\n",
    "                        Supporting text: high accuracy with no bias or fairness issues\n",
    "\n",
    "                        Answer ONLY with one word indicating new estimated impact for this risk (i.e. \"Low\"/\"Medium\"/\"High\"/\"Critical\").\n",
    "                        Do not include any other explanation or information.\n",
    "                        \"\"\"\n",
    "\n",
    "        self.rits_url = f'https://inference-3scale-apicast-production.apps.rits.fmaas.res.ibm.com/{self.model_name}/v1/chat/completions'\n",
    "        \n",
    "    def ask_guardian(self, message):\n",
    "        prompt, risk, priority = message\n",
    "        message = f''' Message:\n",
    "                        AI task: {prompt}\n",
    "                        Risk: {risk}\n",
    "                        Risk impact: {priority}'''\n",
    "\n",
    "    \n",
    "        i = 0\n",
    "        while i < 100:\n",
    "            response = requests.post(\n",
    "                self.rits_url,\n",
    "                headers={\n",
    "                    \"RITS_API_KEY\": os.getenv('RITS_API_KEY'),\n",
    "                    \"Content-Type\": \"application/json\"},\n",
    "                json={\n",
    "                    \"model\": self.model_served_name,\n",
    "                    \"messages\": [{\"role\": \"system\", \"content\": self.context, \"name\":\"test\"}, {\"role\": \"user\", \"content\": message, \"name\":\"test\"}],\n",
    "                    \"temperature\": 0.7, \n",
    "                    \"logprobs\": True, \n",
    "                    \"top_logprobs\": 10\n",
    "                }\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                break\n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "        answer = response.json()['choices'][0]['message']['content']\n",
    "        \n",
    "        return answer\n",
    "\n",
    "    def predict_proba(self, inputs):\n",
    "        probabilities = []\n",
    "\n",
    "        for input_text in inputs:\n",
    "            i = 0\n",
    "            while i < 100:\n",
    "                response = requests.post(\n",
    "                    self.rits_url,\n",
    "                    headers={\n",
    "                        \"RITS_API_KEY\": os.getenv('RITS_API_KEY'),\n",
    "                        \"Content-Type\": \"application/json\"},\n",
    "                    json={\n",
    "                        \"model\": self.model_served_name,\n",
    "                        \"messages\": [{\"role\": \"system\", \"content\": self.context}, {\"role\": \"user\", \"content\": input_text}],\n",
    "                        \"temperature\": 0.7, \n",
    "                        \"logprobs\": True, \n",
    "                        \"top_logprobs\": 10\n",
    "                    }\n",
    "                )\n",
    "                if response.status_code == 200:\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "                                \n",
    "            answer = response.json()['choices'][0]['message']['content']\n",
    "            \n",
    "            log_probs = response.json()['choices'][0]['logprobs']['content'][0]['top_logprobs']\n",
    "\n",
    "            answer = response.json()['choices'][0]\n",
    "    \n",
    "            tokens = [i['token'] for i in log_probs]\n",
    "            token_probs = [i['logprob'] for i in log_probs]\n",
    "            \n",
    "            probs = [math.e ** token_probs[tokens.index(label)] if label in tokens else 0 for label in self.label_names ]\n",
    "            probabilities.append(probs)\n",
    "\n",
    "        return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_config = {\n",
    "                \"task\": \"risk reprioritization\",\n",
    "                 \"criterion\": \"risk impact reevaluation\",\n",
    "                 \"criterion_definition\": \"Given a task, AI risk, and a baseline estimated impact of this risk on the task decide if the impact should be changed.\",\n",
    "                 \"labels\": [0, 1, 2, 3],\n",
    "                 \"label_names\": [\"Low\", \"Medium\", \"High\", \"Critical\"],\n",
    "                 \"output_labels\": [\"Low\", \"Medium\", \"High\", \"Critical\"]\n",
    "              }\n",
    "\n",
    "judge = RiskReprioritizationJudge('llama-3-3-70b-instruct', 'meta-llama/llama-3-3-70b-instruct', judge_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.datasets.prompt_dataset import PromptDataset\n",
    "\n",
    "data_config = {\n",
    "            \"general\": {\n",
    "              \"location\": \"\",\n",
    "              \"dataset_name\": \"intent_dataset\"\n",
    "            },\n",
    "            \"data\": {\n",
    "              \"type\": \"prompt\",\n",
    "              \"index_col\": \"Index\",\n",
    "              \"prompt_col\": \"intent\",\n",
    "              \"label_col\": \"priority\",\n",
    "              \"flip_labels\": False\n",
    "            },\n",
    "            \"split\": {\n",
    "              \"split\": False,\n",
    "              \"subset\": \"test\",\n",
    "              \"sample_ratio\": 1\n",
    "            }\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.datasets.abs_dataset import AbstractDataset\n",
    "\n",
    "class IntentDataset(AbstractDataset):\n",
    "\n",
    "    def __init__(self, config, dataframe=None):\n",
    "        super().__init__(config, dataframe)\n",
    "\n",
    "        self.expl_input = self.prompt_col\n",
    "        self.message_labels = [self.prompt_col, 'risk', 'priority']\n",
    "\n",
    "    def extract_message(self, row):\n",
    "        prompt = row[self.prompt_col]\n",
    "        risk = row['risk']\n",
    "        baseline_priority = row['priority']\n",
    "        local_expl_input = prompt\n",
    "\n",
    "        # get true label\n",
    "        true_label = row[self.label_col]\n",
    "\n",
    "        return (prompt, risk, baseline_priority), local_expl_input, true_label\n",
    "\n",
    "    def build_message_format(self, message, decision):\n",
    "        prompt, risk, _ = message\n",
    "        return f''' Message:\n",
    "                        AI Task: {prompt}\n",
    "                        Risk: {risk}\n",
    "                        Risk impact: {decision}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load intent dataset\n",
    "df = pd.read_csv('datasets/intents.csv', header=0)\n",
    "dataset = IntentDataset(data_config, df[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Built pipeline.\n",
      "INFO:logger:Using LIME = True\n",
      "INFO:logger:Using FactReasoner = True\n",
      "INFO:logger:Loaded concepts from results/intent_dataset/intent_dataset/local/expl.csv\n",
      "/Users/jasmina/Documents/Work/risk_reprioritisation/src/pipeline/pipeline.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cds['Zipped'] = cds.apply(lambda x: self.custom_zipp([ast.literal_eval(x[label_names[d]]) for d in labels]), axis=1)\n",
      "INFO:logger:Loaded the following graph:\n",
      "\tLabels = dict_keys([0, 1, 2, 3]) Sizes = [20, 10, 10, 0] Number of edges = 20\n",
      "/Users/jasmina/Documents/Work/risk_reprioritisation/src/pipeline/pipeline.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cds['Zipped'] = cds.apply(lambda x: self.custom_zipp([ast.literal_eval(x[label_names[d]]) for d in labels]), axis=1)\n",
      "INFO:logger:Loaded the following graph:\n",
      "\tLabels = dict_keys([0, 1, 2, 3]) Sizes = [30, 20, 10, 2] Number of edges = 32\n",
      "INFO:logger:Clustering...\n",
      "INFO:logger:Clustering 30 instances\n",
      "INFO:logger:Cleaned up 9 clusters with 28 total concepts\n",
      "INFO:logger:['data protection', 'data protection', 'data protection', 'data protection', 'data protection', 'data protection']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 6 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 30, probability = data protection, num of subnodes = 1.0\n",
      "INFO:logger:['security verification', 'security verification', 'security verification', 'security verification']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 4 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 31, probability = security verification, num of subnodes = 1.0\n",
      "INFO:logger:['low error likelihood', 'low error likelihood', 'low error likelihood', 'low error likelihood']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 4 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 32, probability = low error likelihood, num of subnodes = 1.0\n",
      "INFO:logger:['robust security verification', 'robust security verification', 'robust security verification', 'robust security verification']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 4 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 33, probability = robust security verification, num of subnodes = 1.0\n",
      "INFO:logger:['not sensitive decisions', 'not sensitive decisions']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 34, probability = not sensitive decisions, num of subnodes = 1.0\n",
      "INFO:logger:['compliant designs', 'compliant designs']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 35, probability = compliant designs, num of subnodes = 1.0\n",
      "INFO:logger:['verified security', 'verified security']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 36, probability = verified security, num of subnodes = 1.0\n",
      "INFO:logger:['respects intellectual property', 'respects intellectual property']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 37, probability = respects intellectual property, num of subnodes = 1.0\n",
      "INFO:logger:['avoids copyright infringement', 'avoids copyright infringement']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 38, probability = avoids copyright infringement, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 20 instances\n",
      "INFO:logger:Cleaned up 6 clusters with 20 total concepts\n",
      "INFO:logger:['verified security measures', 'verified security measures', 'verified security measures', 'verified security measures', 'verified security measures', 'verified security measures', 'verified security measures', 'verified security measures']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 8 nodes on 1 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 20, probability = verified security measures, num of subnodes = 1.0\n",
      "INFO:logger:['mitigating checks', 'mitigating checks', 'mitigating checks', 'mitigating checks']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 4 nodes on 1 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 21, probability = mitigating checks, num of subnodes = 1.0\n",
      "INFO:logger:['mitigated risks', 'mitigated risks']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 1 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 22, probability = mitigated risks, num of subnodes = 1.0\n",
      "INFO:logger:['data protection verified', 'data protection verified']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 1 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 23, probability = data protection verified, num of subnodes = 1.0\n",
      "INFO:logger:['sensitive info access', 'sensitive info access']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 1 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 24, probability = sensitive info access, num of subnodes = 1.0\n",
      "INFO:logger:['mitigated by review process', 'mitigated by review process']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 1 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 25, probability = mitigated by review process, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 10 instances\n",
      "INFO:logger:Cleaned up 3 clusters with 10 total concepts\n",
      "INFO:logger:['financial impact high', 'financial impact high', 'financial impact high', 'financial impact high', 'financial impact high', 'financial impact high']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 6 nodes on 2 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 10, probability = financial impact high, num of subnodes = 1.0\n",
      "INFO:logger:['expense tracking', 'expense tracking']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 2 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 11, probability = expense tracking, num of subnodes = 1.0\n",
      "INFO:logger:['verified security measures', 'verified security measures']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 2 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 12, probability = verified security measures, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 2 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 2 total concepts\n",
      "INFO:logger:['legal harm', 'legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 2, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 11 instances\n",
      "INFO:logger:Iteration = 0\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 11, 1: 6, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.7\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m13:58:21 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m13:58:21 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m13:58:21 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: has robust security verification\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:21 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:21 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:21 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 96791.63prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9265922158856623\n",
      "[c_a0_1 -> a0] : entailment : 0.9130138012804676\n",
      "[c_a0_2 -> a0] : entailment : 0.9693595200871683\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 31068.92it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 99864.38it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 96791.63it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:\n",
      "\t\t\tMerging 3 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 39, probability = has robust security verification, num of subnodes = 0.99593\n",
      "INFO:logger:Named new cluster = \thas robust security verification\n",
      "INFO:logger:\t\tsecurity verification\n",
      "INFO:logger:\t\tverified security\n",
      "INFO:logger:\t\trobust security verification\n",
      "INFO:logger:Clustering 9 instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.00011611 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.00028801 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.887434 (4.117108e-01) \td=8.874342e-01\t time=0.000361\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000363 seconds\n",
      "PR\n",
      "-0.887434 (4.117108e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.004070 0.995930 2 0.101969 0.898031 2 0.101784 0.898216 2 0.102762 0.897238\n",
      "[MERLIN] I/O time is 0.000157 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.00407, 0.99593]}, {'variable': 'c_a0_0', 'probabilities': [0.101969, 0.898031]}, {'variable': 'c_a0_1', 'probabilities': [0.101784, 0.898216]}, {'variable': 'c_a0_2', 'probabilities': [0.102762, 0.897238]}]\n",
      "[a0]: Probability for a0=0 is: 0.00407\n",
      "[a0]: Probability for a0=1 is: 0.99593\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 6 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 3, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 6 instances\n",
      "INFO:logger:Iteration = 1\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 6, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.6849999999999999\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m13:58:34 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m13:58:34 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: has verified security measures\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:35 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:35 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 33288.13prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9580055481498064\n",
      "[c_a0_1 -> a0] : entailment : 0.8112571004657897\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18236.10it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45343.83it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 33288.13it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000114918 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000272989 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.902979 (4.053603e-01) \td=9.029791e-01\t time=0.000342\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000344 seconds\n",
      "PR\n",
      "-0.902979 (4.053603e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.041360 0.958640 2 0.125523 0.874477 2 0.109232 0.890768\n",
      "[MERLIN] I/O time is 0.000177 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.04136, 0.95864]}, {'variable': 'c_a0_0', 'probabilities': [0.125523, 0.874477]}, {'variable': 'c_a0_1', 'probabilities': [0.109232, 0.890768]}]\n",
      "[a0]: Probability for a0=0 is: 0.04136\n",
      "[a0]: Probability for a0=1 is: 0.95864\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:37 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m13:58:37 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: has strong security measures\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:37 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:37 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 39383.14prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9245785644429977\n",
      "[c_a0_1 -> a0] : entailment : 0.7681102190655997\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 8256.50it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 54120.05it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44384.17it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000114202 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000269175 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.972919 (3.779782e-01) \td=9.729188e-01\t time=0.000345\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000348 seconds\n",
      "PR\n",
      "-0.972919 (3.779782e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.060557 0.939443 2 0.128864 0.871136 2 0.110236 0.889764\n",
      "[MERLIN] I/O time is 0.000183 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.060557, 0.939443]}, {'variable': 'c_a0_0', 'probabilities': [0.128864, 0.871136]}, {'variable': 'c_a0_1', 'probabilities': [0.110236, 0.889764]}]\n",
      "[a0]: Probability for a0=0 is: 0.060557\n",
      "[a0]: Probability for a0=1 is: 0.939443\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:39 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m13:58:39 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: has verified protection\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:40 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:40 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 41527.76prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9206138416661792\n",
      "[c_a0_1 -> a0] : entailment : 0.8137309178757234\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20360.70it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 53773.13it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 40721.40it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000158787 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000327826 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.929062 (3.949241e-01) \td=9.290617e-01\t time=0.000396\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000398 seconds\n",
      "PR\n",
      "-0.929062 (3.949241e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.051549 0.948451 2 0.123869 0.876131 2 0.111690 0.888310\n",
      "[MERLIN] I/O time is 0.000172 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.051549, 0.948451]}, {'variable': 'c_a0_0', 'probabilities': [0.123869, 0.876131]}, {'variable': 'c_a0_1', 'probabilities': [0.11169, 0.88831]}]\n",
      "[a0]: Probability for a0=0 is: 0.051549\n",
      "[a0]: Probability for a0=1 is: 0.948451\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:42 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m13:58:42 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: has secure data protection\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:58:42 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m13:58:42 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 25970.92prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9199369092435159\n",
      "[c_a0_1 -> a0] : entailment : 0.9338858556252048\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17331.83it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 51150.05it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44384.17it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 1 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 26, probability = has secure data protection, num of subnodes = 0.971633\n",
      "INFO:logger:Named new cluster = \thas secure data protection\n",
      "INFO:logger:\t\tverified security measures\n",
      "INFO:logger:\t\tdata protection verified\n",
      "INFO:logger:Clustering 9 instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000118971 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000283003 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.816221 (4.420990e-01) \td=8.162215e-01\t time=0.000352\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000353 seconds\n",
      "PR\n",
      "-0.816221 (4.420990e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.028367 0.971633 2 0.113070 0.886930 2 0.114490 0.885510\n",
      "[MERLIN] I/O time is 0.000171 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.028367, 0.971633]}, {'variable': 'c_a0_0', 'probabilities': [0.11307, 0.88693]}, {'variable': 'c_a0_1', 'probabilities': [0.11449, 0.88551]}]\n",
      "[a0]: Probability for a0=0 is: 0.028367\n",
      "[a0]: Probability for a0=1 is: 0.971633\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 4, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Iteration = 2\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.6699999999999999\n",
      "INFO:logger:Generated 0 clusters\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 5, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Iteration = 3\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.6549999999999999\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m13:59:06 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Critical because it contains the following concept: causes legal harm\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 2]\n",
      "[FactReasoner] Found 1 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:59:06 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17772.47prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9692688340075566\n",
      "[Relations built: 1]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24244.53it/s]\n",
      "Contexts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 7244.05it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15592.21it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:Clustering 9 instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 2 nodes and 1 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 0 1 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 8.58307e-05 seconds\n",
      "[WMB] Created join graph with 2 clique factors\n",
      "[WMB] Number of cliques  : 2\n",
      "[WMB] Number of edges    : 2\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.00020504 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.603440 (5.469269e-01) \td=6.034402e-01\t time=0.000252\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000254 seconds\n",
      "PR\n",
      "-0.603440 (5.469269e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "2 2 0.113895 0.886105 2 0.177221 0.822779\n",
      "[MERLIN] I/O time is 0.000158 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.113895, 0.886105]}, {'variable': 'c_a0_0', 'probabilities': [0.177221, 0.822779]}]\n",
      "[a0]: Probability for a0=0 is: 0.113895\n",
      "[a0]: Probability for a0=1 is: 0.886105\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 5/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 6, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Iteration = 4\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.6399999999999999\n",
      "INFO:logger:Generated 0 clusters\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 7, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Iteration = 5\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.6249999999999999\n",
      "INFO:logger:Generated 0 clusters\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 8, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Iteration = 6\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.6099999999999999\n",
      "INFO:logger:Generated 0 clusters\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 9, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Iteration = 7\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.5949999999999999\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m14:00:22 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Critical because it contains the following concept: causes legal harm\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 2]\n",
      "[FactReasoner] Found 1 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:00:22 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14979.66prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9692688340075566\n",
      "[Relations built: 1]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17549.39it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23563.51it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23831.27it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:Clustering 9 instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 2 nodes and 1 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 0 1 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 6.38962e-05 seconds\n",
      "[WMB] Created join graph with 2 clique factors\n",
      "[WMB] Number of cliques  : 2\n",
      "[WMB] Number of edges    : 2\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000144958 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.603440 (5.469269e-01) \td=6.034402e-01\t time=0.000184\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000186 seconds\n",
      "PR\n",
      "-0.603440 (5.469269e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "2 2 0.113895 0.886105 2 0.177221 0.822779\n",
      "[MERLIN] I/O time is 0.000090 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.113895, 0.886105]}, {'variable': 'c_a0_0', 'probabilities': [0.177221, 0.822779]}]\n",
      "[a0]: Probability for a0=0 is: 0.113895\n",
      "[a0]: Probability for a0=1 is: 0.886105\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 5/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 5/5].\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 10, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Iteration = 8\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.5799999999999998\n",
      "INFO:logger:Generated 0 clusters\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 11, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Iteration = 9\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.5649999999999998\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m14:01:34 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:34 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: involves secure data handling\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:35 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:35 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 42581.77prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.864952355142073\n",
      "[c_a0_1 -> a0] : entailment : 0.9021227391814877\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19152.07it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 48770.98it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 36314.32it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 8.51154e-05 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000190973 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.894790 (4.086934e-01) \td=8.947901e-01\t time=0.000241\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000242 seconds\n",
      "PR\n",
      "-0.894790 (4.086934e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.045381 0.954619 2 0.114330 0.885670 2 0.118422 0.881578\n",
      "[MERLIN] I/O time is 0.000127 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.045381, 0.954619]}, {'variable': 'c_a0_0', 'probabilities': [0.11433, 0.88567]}, {'variable': 'c_a0_1', 'probabilities': [0.118422, 0.881578]}]\n",
      "[a0]: Probability for a0=0 is: 0.045381\n",
      "[a0]: Probability for a0=1 is: 0.954619\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:37 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:37 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: involves secure info handling\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:37 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:37 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 39383.14prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8350325447043447\n",
      "[c_a0_1 -> a0] : entailment : 0.9220699092872451\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 50231.19it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 41527.76it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000104189 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000247002 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.906813 (4.038090e-01) \td=9.068133e-01\t time=0.000310\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000313 seconds\n",
      "PR\n",
      "-0.906813 (4.038090e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.046630 0.953370 2 0.112122 0.887878 2 0.121822 0.878178\n",
      "[MERLIN] I/O time is 0.000171 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.04663, 0.95337]}, {'variable': 'c_a0_0', 'probabilities': [0.112122, 0.887878]}, {'variable': 'c_a0_1', 'probabilities': [0.121822, 0.878178]}]\n",
      "[a0]: Probability for a0=0 is: 0.04663\n",
      "[a0]: Probability for a0=1 is: 0.95337\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:39 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:39 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: involves secure info access\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:40 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28055.55prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8893005619492828\n",
      "[c_a0_1 -> a0] : entailment : 0.7927397753535049\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19508.39it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45343.83it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44384.17it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 8.2016e-05 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.00018692 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.974046 (3.775523e-01) \td=9.740462e-01\t time=0.000236\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000238 seconds\n",
      "PR\n",
      "-0.974046 (3.775523e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.066376 0.933624 2 0.124667 0.875333 2 0.113158 0.886842\n",
      "[MERLIN] I/O time is 0.000120 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.066376, 0.933624]}, {'variable': 'c_a0_0', 'probabilities': [0.124667, 0.875333]}, {'variable': 'c_a0_1', 'probabilities': [0.113158, 0.886842]}]\n",
      "[a0]: Probability for a0=0 is: 0.066376\n",
      "[a0]: Probability for a0=1 is: 0.933624\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:41 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: involves secure info\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:42 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:42 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28630.06prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8161533173580057\n",
      "[c_a0_1 -> a0] : entailment : 0.9049209780448925\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17331.83it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 47662.55it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45343.83it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000102043 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000241995 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.938977 (3.910277e-01) \td=9.389768e-01\t time=0.000305\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000308 seconds\n",
      "PR\n",
      "-0.938977 (3.910277e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.055624 0.944376 2 0.112812 0.887188 2 0.123027 0.876973\n",
      "[MERLIN] I/O time is 0.000158 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.055624, 0.944376]}, {'variable': 'c_a0_0', 'probabilities': [0.112812, 0.887188]}, {'variable': 'c_a0_1', 'probabilities': [0.123027, 0.876973]}]\n",
      "[a0]: Probability for a0=0 is: 0.055624\n",
      "[a0]: Probability for a0=1 is: 0.944376\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:44 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:44 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: involves secure info\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:44 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:44 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 37786.52prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8161533173580057\n",
      "[c_a0_1 -> a0] : entailment : 0.9049209780448925\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 30393.51it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 64527.75it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 83055.52it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 8.4877e-05 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000210047 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.938977 (3.910277e-01) \td=9.389768e-01\t time=0.000263\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000265 seconds\n",
      "PR\n",
      "-0.938977 (3.910277e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.055624 0.944376 2 0.112812 0.887188 2 0.123027 0.876973\n",
      "[MERLIN] I/O time is 0.000133 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.055624, 0.944376]}, {'variable': 'c_a0_0', 'probabilities': [0.112812, 0.887188]}, {'variable': 'c_a0_1', 'probabilities': [0.123027, 0.876973]}]\n",
      "[a0]: Probability for a0=0 is: 0.055624\n",
      "[a0]: Probability for a0=1 is: 0.944376\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:46 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:46 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: involves secure data\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:46 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:46 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35098.78prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8631545207930037\n",
      "[c_a0_1 -> a0] : entailment : 0.8728971507853436\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20460.02it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 50840.05it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 51463.85it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000109911 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000263929 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.921689 (3.978464e-01) \td=9.216893e-01\t time=0.000331\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000334 seconds\n",
      "PR\n",
      "-0.921689 (3.978464e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.053095 0.946905 2 0.116569 0.883431 2 0.117671 0.882329\n",
      "[MERLIN] I/O time is 0.000167 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.053095, 0.946905]}, {'variable': 'c_a0_0', 'probabilities': [0.116569, 0.883431]}, {'variable': 'c_a0_1', 'probabilities': [0.117671, 0.882329]}]\n",
      "[a0]: Probability for a0=0 is: 0.053095\n",
      "[a0]: Probability for a0=1 is: 0.946905\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:48 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:48 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: involves private data\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:48 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:48 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 17119.61prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8953951380626907\n",
      "[c_a0_1 -> a0] : entailment : 0.8665886641756727\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16131.94it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 36472.21it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 46345.90it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000118971 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000272036 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.899050 (4.069560e-01) \td=8.990501e-01\t time=0.000337\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000340 seconds\n",
      "PR\n",
      "-0.899050 (4.069560e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.046655 0.953345 2 0.118077 0.881923 2 0.114892 0.885108\n",
      "[MERLIN] I/O time is 0.000166 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.046655, 0.953345]}, {'variable': 'c_a0_0', 'probabilities': [0.118077, 0.881923]}, {'variable': 'c_a0_1', 'probabilities': [0.114892, 0.885108]}]\n",
      "[a0]: Probability for a0=0 is: 0.046655\n",
      "[a0]: Probability for a0=1 is: 0.953345\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: involves secure info access\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:50 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 42366.71prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8893005619492828\n",
      "[c_a0_1 -> a0] : entailment : 0.7927397753535049\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19599.55it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 62601.55it/s]\n",
      "Relations: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 9258.95it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000120878 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000277042 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.974046 (3.775523e-01) \td=9.740462e-01\t time=0.000340\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000342 seconds\n",
      "PR\n",
      "-0.974046 (3.775523e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.066376 0.933624 2 0.124667 0.875333 2 0.113158 0.886842\n",
      "[MERLIN] I/O time is 0.000170 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.066376, 0.933624]}, {'variable': 'c_a0_0', 'probabilities': [0.124667, 0.875333]}, {'variable': 'c_a0_1', 'probabilities': [0.113158, 0.886842]}]\n",
      "[a0]: Probability for a0=0 is: 0.066376\n",
      "[a0]: Probability for a0=1 is: 0.933624\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:52 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:52 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: involves protected info\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:52 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:52 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 35246.25prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8261159578106512\n",
      "[c_a0_1 -> a0] : entailment : 0.8825159357383682\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18558.87it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 40920.04it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44384.17it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000135899 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000305891 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.947466 (3.877221e-01) \td=9.474664e-01\t time=0.000375\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000377 seconds\n",
      "PR\n",
      "-0.947466 (3.877221e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.059816 0.940184 2 0.114685 0.885315 2 0.121231 0.878769\n",
      "[MERLIN] I/O time is 0.000200 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.059816, 0.940184]}, {'variable': 'c_a0_0', 'probabilities': [0.114685, 0.885315]}, {'variable': 'c_a0_1', 'probabilities': [0.121231, 0.878769]}]\n",
      "[a0]: Probability for a0=0 is: 0.059816\n",
      "[a0]: Probability for a0=1 is: 0.940184\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:54 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:01:54 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: has secure data\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:01:54 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:01:54 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44384.17prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7786467931686475\n",
      "[c_a0_1 -> a0] : entailment : 0.8386468935256605\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17476.27it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 47393.27it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 40721.40it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:Clustering 9 instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000123978 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.00029707 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.026546 (3.582422e-01) \td=1.026546e+00\t time=0.000370\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000372 seconds\n",
      "PR\n",
      "-1.026546 (3.582422e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.088592 0.911408 2 0.116037 0.883963 2 0.123573 0.876427\n",
      "[MERLIN] I/O time is 0.000182 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.088592, 0.911408]}, {'variable': 'c_a0_0', 'probabilities': [0.116037, 0.883963]}, {'variable': 'c_a0_1', 'probabilities': [0.123573, 0.876427]}]\n",
      "[a0]: Probability for a0=0 is: 0.088592\n",
      "[a0]: Probability for a0=1 is: 0.911408\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 12, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Iteration = 10\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.5499999999999998\n",
      "INFO:logger:Generated 0 clusters\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 13, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Iteration = 11\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.5349999999999998\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m14:02:19 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Critical because it contains the following concept: may cause legal harm\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 2]\n",
      "[FactReasoner] Found 1 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:02:19 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16644.06prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9486293772993233\n",
      "[Relations built: 1]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14463.12it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20763.88it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17189.77it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:Clustering 9 instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 2 nodes and 1 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 0 1 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 5.91278e-05 seconds\n",
      "[WMB] Created join graph with 2 clique factors\n",
      "[WMB] Number of cliques  : 2\n",
      "[WMB] Number of edges    : 2\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000149012 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.607221 (5.448629e-01) \td=6.072210e-01\t time=0.000188\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000190 seconds\n",
      "PR\n",
      "-0.607221 (5.448629e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "2 2 0.129479 0.870521 2 0.174104 0.825896\n",
      "[MERLIN] I/O time is 0.000161 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.129479, 0.870521]}, {'variable': 'c_a0_0', 'probabilities': [0.174104, 0.825896]}]\n",
      "[a0]: Probability for a0=0 is: 0.129479\n",
      "[a0]: Probability for a0=1 is: 0.870521\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 14, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Iteration = 12\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.5199999999999998\n",
      "INFO:logger:Generated 0 clusters\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 15, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 5 instances\n",
      "INFO:logger:Iteration = 13\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 5, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.5049999999999998\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m14:03:00 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:03:00 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: handles sensitive info securely\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:01 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 42366.71prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7647934586521585\n",
      "[c_a0_1 -> a0] : entailment : 0.9612181280927634\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 47393.27it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44384.17it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000104189 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000257015 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.950765 (3.864452e-01) \td=9.507653e-01\t time=0.000324\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000326 seconds\n",
      "PR\n",
      "-0.950765 (3.864452e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.048852 0.951148 2 0.108080 0.891920 2 0.130953 0.869047\n",
      "[MERLIN] I/O time is 0.000159 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.048852, 0.951148]}, {'variable': 'c_a0_0', 'probabilities': [0.10808, 0.89192]}, {'variable': 'c_a0_1', 'probabilities': [0.130953, 0.869047]}]\n",
      "[a0]: Probability for a0=0 is: 0.048852\n",
      "[a0]: Probability for a0=1 is: 0.951148\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:03 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:03:03 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Protects sensitive info well\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:03 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:03 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 30393.51prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8851236699499542\n",
      "[c_a0_1 -> a0] : entailment : 0.9543515759160242\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37117.73it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 91180.52it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 76260.07it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 8.60691e-05 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000215054 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.831355 (4.354586e-01) \td=8.313555e-01\t time=0.000274\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000275 seconds\n",
      "PR\n",
      "-0.831355 (4.354586e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.030081 0.969919 2 0.110866 0.889134 2 0.118020 0.881980\n",
      "[MERLIN] I/O time is 0.000122 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.030081, 0.969919]}, {'variable': 'c_a0_0', 'probabilities': [0.110866, 0.889134]}, {'variable': 'c_a0_1', 'probabilities': [0.11802, 0.88198]}]\n",
      "[a0]: Probability for a0=0 is: 0.030081\n",
      "[a0]: Probability for a0=1 is: 0.969919\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:05 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:03:05 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: secures sensitive information\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:03:06 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11983.73prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9373050922184932\n",
      "[c_a0_1 -> a0] : entailment : 0.9415431713828323\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4524.60it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 62601.55it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 43464.29it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 1 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 27, probability = secures sensitive information, num of subnodes = 0.975637\n",
      "INFO:logger:Named new cluster = \tsecures sensitive information\n",
      "INFO:logger:\t\tsensitive info access\n",
      "INFO:logger:\t\thas secure data protection\n",
      "INFO:logger:Clustering 9 instances\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000117064 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000292063 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.793464 (4.522755e-01) \td=7.934639e-01\t time=0.000359\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000360 seconds\n",
      "PR\n",
      "-0.793464 (4.522755e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.024363 0.975637 2 0.112772 0.887228 2 0.113193 0.886807\n",
      "[MERLIN] I/O time is 0.000180 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.024363, 0.975637]}, {'variable': 'c_a0_0', 'probabilities': [0.112772, 0.887228]}, {'variable': 'c_a0_1', 'probabilities': [0.113193, 0.886807]}]\n",
      "[a0]: Probability for a0=0 is: 0.024363\n",
      "[a0]: Probability for a0=1 is: 0.975637\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 5/5].\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 4 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 16, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Iteration = 14\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 4, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.48999999999999977\n",
      "INFO:logger:Generated 0 clusters\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 4 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 17, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Iteration = 15\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 4, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.47499999999999976\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m14:03:52 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Critical because it contains the following concept: may cause legal harm\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 2]\n",
      "[FactReasoner] Found 1 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:03:53 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 16320.25prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9486293772993233\n",
      "[Relations built: 1]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13530.01it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24966.10it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 21732.15it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:Clustering 9 instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 2 nodes and 1 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 0 1 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 5.29289e-05 seconds\n",
      "[WMB] Created join graph with 2 clique factors\n",
      "[WMB] Number of cliques  : 2\n",
      "[WMB] Number of edges    : 2\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000126123 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.607221 (5.448629e-01) \td=6.072210e-01\t time=0.000160\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000161 seconds\n",
      "PR\n",
      "-0.607221 (5.448629e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "2 2 0.129479 0.870521 2 0.174104 0.825896\n",
      "[MERLIN] I/O time is 0.000124 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.129479, 0.870521]}, {'variable': 'c_a0_0', 'probabilities': [0.174104, 0.825896]}]\n",
      "[a0]: Probability for a0=0 is: 0.129479\n",
      "[a0]: Probability for a0=1 is: 0.870521\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 4 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 18, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 9 instances\n",
      "INFO:logger:Iteration = 16\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 9, 1: 4, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.45999999999999974\n",
      "INFO:logger:Generated 2 clusters\n",
      "\u001b[92m14:04:05 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:05 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: has robust security measures\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:06 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:06 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 36314.32prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7190811171276842\n",
      "[c_a0_1 -> a0] : entailment : 0.9637960906153816\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18477.11it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 66576.25it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 20068.44it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000117064 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000267982 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.001132 (3.674632e-01) \td=1.001132e+00\t time=0.000336\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000338 seconds\n",
      "PR\n",
      "-1.001132 (3.674632e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.056984 0.943016 2 0.106920 0.893080 2 0.136888 0.863112\n",
      "[MERLIN] I/O time is 0.000167 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.056984, 0.943016]}, {'variable': 'c_a0_0', 'probabilities': [0.10692, 0.89308]}, {'variable': 'c_a0_1', 'probabilities': [0.136888, 0.863112]}]\n",
      "[a0]: Probability for a0=0 is: 0.056984\n",
      "[a0]: Probability for a0=1 is: 0.943016\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:07 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:07 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: uses strong security checks\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:08 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 28149.69prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7295998617850413\n",
      "[c_a0_1 -> a0] : entailment : 0.8804538680154871\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23172.95it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 55924.05it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 52428.80it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.00011301 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000277042 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.043751 (3.521315e-01) \td=1.043751e+00\t time=0.000341\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000343 seconds\n",
      "PR\n",
      "-1.043751 (3.521315e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.087870 0.912130 2 0.111480 0.888520 2 0.130759 0.869241\n",
      "[MERLIN] I/O time is 0.000188 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.08787, 0.91213]}, {'variable': 'c_a0_0', 'probabilities': [0.11148, 0.88852]}, {'variable': 'c_a0_1', 'probabilities': [0.130759, 0.869241]}]\n",
      "[a0]: Probability for a0=0 is: 0.08787\n",
      "[a0]: Probability for a0=1 is: 0.91213\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:10 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:10 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: Has strong security features\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:10 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:10 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 49932.19prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.75063738675061\n",
      "[c_a0_1 -> a0] : entailment : 0.9105012895357295\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 52758.54it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 32768.00it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.00011301 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000272989 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.001238 (3.674242e-01) \td=1.001238e+00\t time=0.000341\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000343 seconds\n",
      "PR\n",
      "-1.001238 (3.674242e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.069936 0.930064 2 0.110535 0.889465 2 0.130114 0.869886\n",
      "[MERLIN] I/O time is 0.000170 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.069936, 0.930064]}, {'variable': 'c_a0_0', 'probabilities': [0.110535, 0.889465]}, {'variable': 'c_a0_1', 'probabilities': [0.130114, 0.869886]}]\n",
      "[a0]: Probability for a0=0 is: 0.069936\n",
      "[a0]: Probability for a0=1 is: 0.930064\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:13 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:13 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: uses strong security checks\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:13 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:13 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 48489.06prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7295970507923555\n",
      "[c_a0_1 -> a0] : entailment : 0.8804540429289215\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 55553.70it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 41527.76it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000114918 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000280857 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.043753 (3.521305e-01) \td=1.043753e+00\t time=0.000350\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000352 seconds\n",
      "PR\n",
      "-1.043753 (3.521305e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.087871 0.912129 2 0.111480 0.888520 2 0.130759 0.869241\n",
      "[MERLIN] I/O time is 0.000183 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.087871, 0.912129]}, {'variable': 'c_a0_0', 'probabilities': [0.11148, 0.88852]}, {'variable': 'c_a0_1', 'probabilities': [0.130759, 0.869241]}]\n",
      "[a0]: Probability for a0=0 is: 0.087871\n",
      "[a0]: Probability for a0=1 is: 0.912129\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: employs strong security protocols\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:15 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 47393.27prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8067816185132712\n",
      "[c_a0_1 -> a0] : entailment : 0.9195470326431171\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 50840.05it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 51150.05it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000127077 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000283957 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.936856 (3.918581e-01) \td=9.368556e-01\t time=0.000354\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000357 seconds\n",
      "PR\n",
      "-0.936856 (3.918581e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.053390 0.946610 2 0.111581 0.888419 2 0.124531 0.875469\n",
      "[MERLIN] I/O time is 0.000186 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.05339, 0.94661]}, {'variable': 'c_a0_0', 'probabilities': [0.111581, 0.888419]}, {'variable': 'c_a0_1', 'probabilities': [0.124531, 0.875469]}]\n",
      "[a0]: Probability for a0=0 is: 0.05339\n",
      "[a0]: Probability for a0=1 is: 0.94661\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: uses strong security checks\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:17 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 39199.10prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7295970507923555\n",
      "[c_a0_1 -> a0] : entailment : 0.8804540429289215\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26214.40it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 71089.90it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 64527.75it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000121117 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000282049 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.043753 (3.521305e-01) \td=1.043753e+00\t time=0.000359\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000362 seconds\n",
      "PR\n",
      "-1.043753 (3.521305e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.087871 0.912129 2 0.111480 0.888520 2 0.130759 0.869241\n",
      "[MERLIN] I/O time is 0.000167 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.087871, 0.912129]}, {'variable': 'c_a0_0', 'probabilities': [0.11148, 0.88852]}, {'variable': 'c_a0_1', 'probabilities': [0.130759, 0.869241]}]\n",
      "[a0]: Probability for a0=0 is: 0.087871\n",
      "[a0]: Probability for a0=1 is: 0.912129\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:18 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:18 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: Verifies user data safely\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:18 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:18 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 42581.77prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8450009803300116\n",
      "[c_a0_1 -> a0] : entailment : 0.8565885404307143\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18157.16it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 50840.05it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 60349.70it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000117779 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000274897 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.952025 (3.859587e-01) \td=9.520250e-01\t time=0.000339\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000341 seconds\n",
      "PR\n",
      "-0.952025 (3.859587e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.062311 0.937689 2 0.117275 0.882725 2 0.118626 0.881374\n",
      "[MERLIN] I/O time is 0.000179 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.062311, 0.937689]}, {'variable': 'c_a0_0', 'probabilities': [0.117275, 0.882725]}, {'variable': 'c_a0_1', 'probabilities': [0.118626, 0.881374]}]\n",
      "[a0]: Probability for a0=0 is: 0.062311\n",
      "[a0]: Probability for a0=1 is: 0.937689\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:20 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:20 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: secures user data\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:20 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:20 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 44384.17prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9298921962862833\n",
      "[c_a0_1 -> a0] : entailment : 0.8779896045624326\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19152.07it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 48770.98it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 47662.55it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000120163 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000271082 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.858872 (4.236398e-01) \td=8.588717e-01\t time=0.000340\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000343 seconds\n",
      "PR\n",
      "-0.858872 (4.236398e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.036403 0.963597 2 0.118047 0.881953 2 0.112534 0.887466\n",
      "[MERLIN] I/O time is 0.000164 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.036403, 0.963597]}, {'variable': 'c_a0_0', 'probabilities': [0.118047, 0.881953]}, {'variable': 'c_a0_1', 'probabilities': [0.112534, 0.887466]}]\n",
      "[a0]: Probability for a0=0 is: 0.036403\n",
      "[a0]: Probability for a0=1 is: 0.963597\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:22 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:22 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: Verifies user data\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:22 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:22 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 33288.13prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7703191923648464\n",
      "[c_a0_1 -> a0] : entailment : 0.761975778918355\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17260.51it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 38479.85it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 83886.08it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 8.79765e-05 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000224829 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.094566 (3.346848e-01) \td=1.094566e+00\t time=0.000275\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000276 seconds\n",
      "PR\n",
      "-1.094566 (3.346848e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.123108 0.876892 2 0.121111 0.878889 2 0.119989 0.880011\n",
      "[MERLIN] I/O time is 0.000120 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.123108, 0.876892]}, {'variable': 'c_a0_0', 'probabilities': [0.121111, 0.878889]}, {'variable': 'c_a0_1', 'probabilities': [0.119989, 0.880011]}]\n",
      "[a0]: Probability for a0=0 is: 0.123108\n",
      "[a0]: Probability for a0=1 is: 0.876892\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: Verifies data safely always\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:24 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 41734.37prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7520736470027967\n",
      "[c_a0_1 -> a0] : entailment : 0.8643637617808536\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 54120.05it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 43464.29it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000113964 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000261068 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.032443 (3.561360e-01) \td=1.032443e+00\t time=0.000346\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000348 seconds\n",
      "PR\n",
      "-1.032443 (3.561360e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.087336 0.912664 2 0.113282 0.886718 2 0.127471 0.872529\n",
      "[MERLIN] I/O time is 0.000166 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.087336, 0.912664]}, {'variable': 'c_a0_0', 'probabilities': [0.113282, 0.886718]}, {'variable': 'c_a0_1', 'probabilities': [0.127471, 0.872529]}]\n",
      "[a0]: Probability for a0=0 is: 0.087336\n",
      "[a0]: Probability for a0=1 is: 0.912664\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:26 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:26 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Low because it contains the following concept: respects intellectual property\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 4]\n",
      "[FactReasoner] Found 2 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:26 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:26 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 39199.10prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9510627696445406\n",
      "[c_a0_1 -> a0] : entailment : 0.9267792409189967\n",
      "[Relations built: 2]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19239.93it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 45343.83it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 46345.90it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:\n",
      "\t\t\tMerging 2 nodes on 0 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 40, probability = respects intellectual property, num of subnodes = 0.975577\n",
      "INFO:logger:Named new cluster = \trespects intellectual property\n",
      "INFO:logger:\t\trespects intellectual property\n",
      "INFO:logger:\t\tavoids copyright infringement\n",
      "INFO:logger:Clustering 8 instances\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 3 nodes and 2 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 9.29832e-05 seconds\n",
      "[WMB] Created join graph with 3 clique factors\n",
      "[WMB] Number of cliques  : 3\n",
      "[WMB] Number of edges    : 4\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000226974 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.794636 (4.517456e-01) \td=7.946362e-01\t time=0.000284\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000286 seconds\n",
      "PR\n",
      "-0.794636 (4.517456e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "3 2 0.024423 0.975577 2 0.114250 0.885750 2 0.111831 0.888169\n",
      "[MERLIN] I/O time is 0.000113 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.024423, 0.975577]}, {'variable': 'c_a0_0', 'probabilities': [0.11425, 0.88575]}, {'variable': 'c_a0_1', 'probabilities': [0.111831, 0.888169]}]\n",
      "[a0]: Probability for a0=0 is: 0.024423\n",
      "[a0]: Probability for a0=1 is: 0.975577\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 2s [Retry 2/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 4s [Retry 3/5].\n",
      "HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "WARNING:huggingface_hub.utils._http:HTTP Error 429 thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "WARNING:huggingface_hub.utils._http:Retrying in 8s [Retry 4/5].\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 4 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 19, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 4 instances\n",
      "INFO:logger:Iteration = 17\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 8, 1: 4, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.44499999999999973\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m14:04:53 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:53 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:53 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Has security checks\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:54 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:54 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:54 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 61082.10prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : contradiction : 0.715610124994271\n",
      "[c_a0_1 -> a0] : contradiction : 0.6961830612897959\n",
      "[c_a0_2 -> a0] : entailment : 0.8058302542063172\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20068.44it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 72733.60it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 93902.33it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (contradiction)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (contradiction)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000117779 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000290871 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -2.217575 (1.088728e-01) \td=2.217575e+00\t time=0.000365\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000367 seconds\n",
      "PR\n",
      "-2.217575 (1.088728e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.584201 0.415799 2 0.149272 0.850728 2 0.142801 0.857199 2 0.225952 0.774048\n",
      "[MERLIN] I/O time is 0.000176 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.584201, 0.415799]}, {'variable': 'c_a0_0', 'probabilities': [0.149272, 0.850728]}, {'variable': 'c_a0_1', 'probabilities': [0.142801, 0.857199]}, {'variable': 'c_a0_2', 'probabilities': [0.225952, 0.774048]}]\n",
      "[a0]: Probability for a0=0 is: 0.584201\n",
      "[a0]: Probability for a0=1 is: 0.415799\n",
      "[FactReasoner] Predictions:  a0: NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:55 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:55 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:55 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Has internal safety measures\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:56 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 58798.65prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7397560914812191\n",
      "[c_a0_1 -> a0] : entailment : 0.7121694130020796\n",
      "[c_a0_2 -> a0] : entailment : 0.6919332531302596\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 23301.69it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 68385.39it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 66576.25it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000134945 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000334024 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.609947 (1.998982e-01) \td=1.609947e+00\t time=0.000421\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000423 seconds\n",
      "PR\n",
      "-1.609947 (1.998982e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.088205 0.911795 2 0.112351 0.887649 2 0.110200 0.889800 2 0.108796 0.891204\n",
      "[MERLIN] I/O time is 0.000206 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.088205, 0.911795]}, {'variable': 'c_a0_0', 'probabilities': [0.112351, 0.887649]}, {'variable': 'c_a0_1', 'probabilities': [0.1102, 0.8898]}, {'variable': 'c_a0_2', 'probabilities': [0.108796, 0.891204]}]\n",
      "[a0]: Probability for a0=0 is: 0.088205\n",
      "[a0]: Probability for a0=1 is: 0.911795\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:57 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:57 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:04:57 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Has safety review process\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:04:58 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 61380.06prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7708151747305964\n",
      "[c_a0_1 -> a0] : entailment : 0.8619903262135796\n",
      "[c_a0_2 -> a0] : entailment : 0.7052423090957837\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26886.56it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 77195.78it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 93206.76it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.00011301 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000285149 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.409342 (2.443041e-01) \td=1.409342e+00\t time=0.000362\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000367 seconds\n",
      "PR\n",
      "-1.409342 (2.443041e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.040974 0.959026 2 0.107049 0.892951 2 0.112689 0.887311 2 0.104508 0.895492\n",
      "[MERLIN] I/O time is 0.000168 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.040974, 0.959026]}, {'variable': 'c_a0_0', 'probabilities': [0.107049, 0.892951]}, {'variable': 'c_a0_1', 'probabilities': [0.112689, 0.887311]}, {'variable': 'c_a0_2', 'probabilities': [0.104508, 0.895492]}]\n",
      "[a0]: Probability for a0=0 is: 0.040974\n",
      "[a0]: Probability for a0=1 is: 0.959026\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:00 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:00 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:00 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Has internal safety checks\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:00 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:00 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:00 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 73156.47prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8022122512230093\n",
      "[c_a0_1 -> a0] : entailment : 0.7222801105761157\n",
      "[c_a0_2 -> a0] : contradiction : 0.740924303194122\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 21546.08it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 85598.04it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (contradiction)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000145912 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000334978 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -2.121618 (1.198376e-01) \td=2.121618e+00\t time=0.000442\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000446 seconds\n",
      "PR\n",
      "-2.121618 (1.198376e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.257188 0.742812 2 0.154179 0.845821 2 0.131940 0.868060 2 0.204840 0.795160\n",
      "[MERLIN] I/O time is 0.000181 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.257188, 0.742812]}, {'variable': 'c_a0_0', 'probabilities': [0.154179, 0.845821]}, {'variable': 'c_a0_1', 'probabilities': [0.13194, 0.86806]}, {'variable': 'c_a0_2', 'probabilities': [0.20484, 0.79516]}]\n",
      "[a0]: Probability for a0=0 is: 0.257188\n",
      "[a0]: Probability for a0=1 is: 0.742812\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:02 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:02 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:02 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Has secure review process\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:02 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:02 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:02 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 63872.65prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7242456003865526\n",
      "[c_a0_1 -> a0] : entailment : 0.7817538467858279\n",
      "[c_a0_2 -> a0] : entailment : 0.8376846577776922\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4466.78it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 91180.52it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 73156.47it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000130177 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.00032711 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.397320 (2.472586e-01) \td=1.397320e+00\t time=0.000414\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000419 seconds\n",
      "PR\n",
      "-1.397320 (2.472586e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.040920 0.959080 2 0.105152 0.894848 2 0.107557 0.892443 2 0.110821 0.889179\n",
      "[MERLIN] I/O time is 0.000195 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.04092, 0.95908]}, {'variable': 'c_a0_0', 'probabilities': [0.105152, 0.894848]}, {'variable': 'c_a0_1', 'probabilities': [0.107557, 0.892443]}, {'variable': 'c_a0_2', 'probabilities': [0.110821, 0.889179]}]\n",
      "[a0]: Probability for a0=0 is: 0.04092\n",
      "[a0]: Probability for a0=1 is: 0.95908\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:04 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:04 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:04 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Has information security\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:04 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:04 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:04 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 119837.26prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7377412991606347\n",
      "[c_a0_1 -> a0] : contradiction : 0.7151878791492116\n",
      "[c_a0_2 -> a0] : entailment : 0.9036791826688213\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 29330.80it/s]\n",
      "Contexts: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 129720.74it/s]\n",
      "Relations: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 136770.78it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (contradiction)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000109911 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000267029 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -2.048711 (1.289009e-01) \td=2.048711e+00\t time=0.000343\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000346 seconds\n",
      "PR\n",
      "-2.048711 (1.289009e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.152173 0.847827 2 0.121020 0.878980 2 0.200167 0.799833 2 0.162450 0.837550\n",
      "[MERLIN] I/O time is 0.000150 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.152173, 0.847827]}, {'variable': 'c_a0_0', 'probabilities': [0.12102, 0.87898]}, {'variable': 'c_a0_1', 'probabilities': [0.200167, 0.799833]}, {'variable': 'c_a0_2', 'probabilities': [0.16245, 0.83755]}]\n",
      "[a0]: Probability for a0=0 is: 0.152173\n",
      "[a0]: Probability for a0=1 is: 0.847827\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Secures sensitive info\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:06 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 63550.06prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : contradiction : 0.7376499116124107\n",
      "[c_a0_1 -> a0] : contradiction : 0.7649158131376538\n",
      "[c_a0_2 -> a0] : entailment : 0.8794277112775811\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20360.70it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 83330.54it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 68759.08it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (contradiction)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (contradiction)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000133991 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000329971 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -2.357291 (9.467633e-02) \td=2.357291e+00\t time=0.000416\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000418 seconds\n",
      "PR\n",
      "-2.357291 (9.467633e-02)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.585412 0.414588 2 0.157231 0.842769 2 0.168628 0.831372 2 0.303514 0.696486\n",
      "[MERLIN] I/O time is 0.000191 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.585412, 0.414588]}, {'variable': 'c_a0_0', 'probabilities': [0.157231, 0.842769]}, {'variable': 'c_a0_1', 'probabilities': [0.168628, 0.831372]}, {'variable': 'c_a0_2', 'probabilities': [0.303514, 0.696486]}]\n",
      "[a0]: Probability for a0=0 is: 0.585412\n",
      "[a0]: Probability for a0=1 is: 0.414588\n",
      "[FactReasoner] Predictions:  a0: NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:08 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:08 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:08 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Has safety review checks\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:08 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:08 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:08 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 68015.74prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.8024061984275213\n",
      "[c_a0_1 -> a0] : entailment : 0.8357594296701782\n",
      "[c_a0_2 -> a0] : entailment : 0.7382430939476882\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19972.88it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 64198.53it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 68015.74it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.00011301 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000284195 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.359543 (2.567781e-01) \td=1.359543e+00\t time=0.000363\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000365 seconds\n",
      "PR\n",
      "-1.359543 (2.567781e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.035978 0.964022 2 0.107588 0.892412 2 0.109397 0.890603 2 0.104987 0.895013\n",
      "[MERLIN] I/O time is 0.000149 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.035978, 0.964022]}, {'variable': 'c_a0_0', 'probabilities': [0.107588, 0.892412]}, {'variable': 'c_a0_1', 'probabilities': [0.109397, 0.890603]}, {'variable': 'c_a0_2', 'probabilities': [0.104987, 0.895013]}]\n",
      "[a0]: Probability for a0=0 is: 0.035978\n",
      "[a0]: Probability for a0=1 is: 0.964022\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:11 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:11 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:11 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Uses security review measures\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:12 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:12 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:12 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 56679.78prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.7415906732381649\n",
      "[c_a0_1 -> a0] : entailment : 0.7493551309046996\n",
      "[c_a0_2 -> a0] : entailment : 0.7798759349230535\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18893.26it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 71493.82it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 66576.25it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000105143 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000281096 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -1.472204 (2.294192e-01) \td=1.472204e+00\t time=0.000355\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000357 seconds\n",
      "PR\n",
      "-1.472204 (2.294192e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.055466 0.944534 2 0.107864 0.892136 2 0.108284 0.891716 2 0.110120 0.889880\n",
      "[MERLIN] I/O time is 0.000133 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.055466, 0.944534]}, {'variable': 'c_a0_0', 'probabilities': [0.107864, 0.892136]}, {'variable': 'c_a0_1', 'probabilities': [0.108284, 0.891716]}, {'variable': 'c_a0_2', 'probabilities': [0.11012, 0.88988]}]\n",
      "[a0]: Probability for a0=0 is: 0.055466\n",
      "[a0]: Probability for a0=1 is: 0.944534\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:14 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:14 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "\u001b[92m14:05:14 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Medium because it contains the following concept: Has safety review\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 6]\n",
      "[FactReasoner] Found 3 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:14 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:14 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:05:14 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 63872.65prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : contradiction : 0.7344649532970994\n",
      "[c_a0_1 -> a0] : entailment : 0.7476550879556477\n",
      "[c_a0_2 -> a0] : contradiction : 0.7038656240034349\n",
      "[Relations built: 3]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 21290.88it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 73156.47it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 66930.38it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:Clustering 8 instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding context variable c_a0_1 with discrete factor (prior)\n",
      "Adding context variable c_a0_2 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (contradiction)\n",
      "Adding edge c_a0_1 - a0 with discrete factor (entailment)\n",
      "Adding edge c_a0_2 - a0 with discrete factor (contradiction)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 4 nodes and 3 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 1 2 3 0 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 0.000145912 seconds\n",
      "[WMB] Created join graph with 4 clique factors\n",
      "[WMB] Number of cliques  : 4\n",
      "[WMB] Number of edges    : 6\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000363827 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -2.142352 (1.173784e-01) \td=2.142352e+00\t time=0.000450\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000452 seconds\n",
      "PR\n",
      "-2.142352 (1.173784e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "4 2 0.664769 0.335231 2 0.145284 0.854716 2 0.198166 0.801834 2 0.136513 0.863487\n",
      "[MERLIN] I/O time is 0.000195 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.664769, 0.335231]}, {'variable': 'c_a0_0', 'probabilities': [0.145284, 0.854716]}, {'variable': 'c_a0_1', 'probabilities': [0.198166, 0.801834]}, {'variable': 'c_a0_2', 'probabilities': [0.136513, 0.863487]}]\n",
      "[a0]: Probability for a0=0 is: 0.664769\n",
      "[a0]: Probability for a0=1 is: 0.335231\n",
      "[FactReasoner] Predictions:  a0: NS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 4 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 20, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Iteration = 18\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 8, 1: 4, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.4299999999999997\n",
      "INFO:logger:Generated 0 clusters\n",
      "INFO:logger:Clustering 8 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 4 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 3 instances\n",
      "INFO:logger:Cleaned up 0 clusters with 0 total concepts\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Cleaned up 1 clusters with 1 total concepts\n",
      "INFO:logger:['legal harm']\n",
      "INFO:logger:\n",
      "\t\t\tMerging 1 nodes on 3 side.\n",
      "INFO:logger:\t\tAdded a node: id = label = 21, probability = legal harm, num of subnodes = 1.0\n",
      "INFO:logger:Clustering 1 instances\n",
      "INFO:logger:Iteration = 19\n",
      "INFO:logger:\tNumber of nodes in graph: {0: 8, 1: 4, 2: 3, 3: 1}\n",
      "INFO:logger:\tUsing threshold = 0.4149999999999997\n",
      "INFO:logger:Generated 1 clusters\n",
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: utils.py:3341 - \n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= meta-llama/Llama-3.1-8B-Instruct; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AtomReviser] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[AtomReviser] Using prompt version: v1\n",
      "[NLIExtractor] Using LLM on RITS: llama-3.1-8b-instruct\n",
      "[NLIExtractor] Prompt version: v1\n",
      "[FactReasoner] Using merlin at: /Users/jasmina/Documents/Work/fm-factual/merlin/bin/merlin\n",
      "[FactReasoner] Using atom/context priors: False\n",
      "[FactReasoner] Building the pipeline instance ...\n",
      "[FactReasoner] Using text only contexts: True\n",
      "[Building atoms ...]\n",
      "Atom a0: The text was classified as Critical because it contains the following concept: causes legal harm\n",
      "[Atoms built: 1]\n",
      "[Building contexts...]\n",
      "[Contexts built: 2]\n",
      "[FactReasoner] Found 1 unique contexts.\n",
      "[Building atom-context relations...]\n",
      "Using all contexts retrieved per atom.\n",
      "[NLIExtractor] Prompts created: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:05:38 - LiteLLM:INFO\u001b[0m: utils.py:1274 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n",
      "NLI: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 12985.46prompts/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[c_a0_0 -> a0] : entailment : 0.9692688340075566\n",
      "[Relations built: 1]\n",
      "[FactReasoner] Building the graphical model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Atoms: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 9986.44it/s]\n",
      "Contexts: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17189.77it/s]\n",
      "Relations: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22192.08it/s]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "INFO:logger:Loaded 2 rules\n",
      "INFO:logger:Stored explanation at results/intent_dataset/intent_dataset/global/global_expl.pkl\n",
      "INFO:logger:Saved the explanation at results/intent_dataset/intent_dataset/global/global_expl.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building the Markov network...]\n",
      "Adding atom variable a0 with discrete factor (prior)\n",
      "Adding context variable c_a0_0 with discrete factor (prior)\n",
      "Adding edge c_a0_0 - a0 with discrete factor (entailment)\n",
      "[Markov network created.]\n",
      "MarkovNetwork with 2 nodes and 1 edges\n",
      "[FactReasoner] Pipeline instance created.\n",
      "libmerlin 1.7.0\n",
      "(c) Copyright IBM Corp. 2015 - 2019\n",
      "All Rights Reserved\n",
      "[MERLIN] Initialize Merlin engine ...\n",
      "[MERLIN] + tasks supported  : PR, MAR, MAP, MMAP, EM\n",
      "[WMB] + i-bound          : 6\n",
      "[WMB] + iterations       : 10\n",
      "[WMB] + inference task   : MAR\n",
      "[WMB] + ordering method  : MinFill\n",
      "[WMB] + order iterations : 100\n",
      "[WMB] + elimination      : 0 1 \n",
      "[WMB] + induced width    : 1\n",
      "[WMB] + exact inference  : Yes\n",
      "[WMB] + ordering time    : 8.01086e-05 seconds\n",
      "[WMB] Created join graph with 2 clique factors\n",
      "[WMB] Number of cliques  : 2\n",
      "[WMB] Number of edges    : 2\n",
      "[WMB] Max clique size    : 2\n",
      "[WMB] Max separator size : 1\n",
      "[WMB] Finished initialization in 0.000178099 seconds\n",
      "[WMB] Begin message passing over join graph ...\n",
      "  logZ:    -0.603440 (5.469269e-01) \td=6.034402e-01\t time=0.000227\ti=1\n",
      "[WMB] Converged after 1 iterations in 0.000229 seconds\n",
      "PR\n",
      "-0.603440 (5.469269e-01)\n",
      "STATUS\n",
      "true: Consistent evidence\n",
      "MAR\n",
      "2 2 0.113895 0.886105 2 0.177221 0.822779\n",
      "[MERLIN] I/O time is 0.000142 seconds\n",
      "[Merlin] return code: 0\n",
      "All Marginals:\n",
      "[{'variable': 'a0', 'probabilities': [0.113895, 0.886105]}, {'variable': 'c_a0_0', 'probabilities': [0.177221, 0.822779]}]\n",
      "[a0]: Probability for a0=0 is: 0.113895\n",
      "[a0]: Probability for a0=1 is: 0.886105\n",
      "[FactReasoner] Predictions:  a0: S\n"
     ]
    }
   ],
   "source": [
    "from src.models.local_explainers.lime import LIME\n",
    "from src.pipeline.pipeline import Pipeline\n",
    "from src.pipeline.clusterer import Clusterer\n",
    "from src.pipeline.concept_extractor import Extractor\n",
    "\n",
    "llm_component = RITSComponent('llama-3-3-70b-instruct', 'meta-llama/llama-3-3-70b-instruct')\n",
    "local_explainer = LIME('intent', judge_config['label_names'], n_samples=500, n_words=6)\n",
    "\n",
    "pipeline = Pipeline(extractor=Extractor(judge, llm_component, judge_config['criterion'],\n",
    "                                        judge_config['criterion_definition'], local_explainer),\n",
    "                    clusterer=Clusterer(llm_component, judge_config['criterion_definition'],\n",
    "                                        judge_config['label_names'], n_iter=20),\n",
    "                    lime=True,\n",
    "                    fr=True,\n",
    "                    verbose=1)\n",
    "\n",
    "# evaluate the global explanation\n",
    "path = f'results/intent_dataset'\n",
    "expl = pipeline.run(dataset, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 IF secures sensitive information DESPITE ['has robust security verification']\n",
      "0 IF has robust security verification DESPITE ['secures sensitive information']\n"
     ]
    }
   ],
   "source": [
    "for i, r in enumerate(expl.rules):\n",
    "    print(f'{expl.predictions[i]} IF {r} DESPITE {expl.despites[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
