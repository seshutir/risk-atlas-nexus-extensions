import ast
import itertools
import logging
import os

import pandas as pd

from risk_policy_distillation.datasets.abs_dataset import AbstractDataset
from risk_policy_distillation.explanation.bipartite_graph import BipartiteGraph
from risk_policy_distillation.models.explainers.global_explainers.gelpe import Gelpe
from risk_policy_distillation.models.explainers.global_explainers.global_expl import (
    GlobalExplainer,
)
from risk_policy_distillation.pipeline.clusterer import Clusterer
from risk_policy_distillation.pipeline.concept_extractor import Extractor


logger = logging.getLogger("logger")


class Pipeline:

    def __init__(
        self,
        extractor=None,
        clusterer=None,
        concept_dataset=None,
        lime=True,
        fr=True,
        verbose=0,
    ):
        """
        Pipeline for generating local and global explanations.
        :param extractor: an Extractor object representing local explanation generation
        :param clusterer: a Clusterer object representing global explanation generation
        :param concept_dataset: a AbstractDataset object
        :param lime: If the pipeline uses a local word-based explainer like LIME to verify local concept-based explanations
        :param fr: If the pipeline uses a factuality assessor like FactReasoner to verify global explanations
        :param verbose: amount of logs generated -- 0 for very little logging, 1 for practically everything logged.
        """
        self.extractor = extractor
        self.clusterer = clusterer

        self.concept_dataset = concept_dataset
        self.lime = lime
        self.fr = fr

        self.verbose = verbose

        logger.info("Built pipeline.")
        logger.info("Using LIME = {}".format(self.lime))
        logger.info("Using FactReasoner = {}".format(self.fr))

    # TODO: think about passing the guardian model to the run() instead of through Extractor
    def run(self, dataset: AbstractDataset, path="results/") -> GlobalExplainer:
        """
        Runs the full pipeline for local and global explanation generation.
        :param dataset: an AbstractDataset or one of its subclasses
        :param path: a global path to the results folder within which local and global explanations will be saved.
        :return: global explanation for the dataset generated by the pipeline.
        """
        # generate paths for saving the explanations
        local_expl_path, global_expl_path = self.generate_output_folders(
            path, dataset.dataset_name
        )

        # if the global explanation already exists load it
        if os.path.exists(global_expl_path):
            global_expl = GlobalExplainer(expl_path=global_expl_path)
            logger.info("Loaded graph explanation from {}".format(global_expl_path))
            return global_expl

        if self.concept_dataset is not None:
            self.concept_dataset = self.concept_dataset
        elif self.extractor is not None:
            # generate local explanations by extracting concepts
            self.extractor.extract_concepts(
                dataset, local_expl_path, self.lime, self.verbose
            )
            self.concept_dataset = pd.read_csv(local_expl_path, header=0)
        else:
            raise ValueError(
                "No extractor or concept dataset provided for the pipeline."
            )

        labels = self.extractor.guardian.labels
        # load concepts into a bipartite graph format
        expl_graph = self.get_graph_expl(self.concept_dataset, labels)

        # run clustering to reduce the size of the graph
        if self.clusterer is None:
            return None

        best_graph = self.clusterer.run_clustering(
            expl_graph, labels, self.fr, self.verbose
        )

        # get a global explanation from the summarized graph
        global_expl = GlobalExplainer(expl_graph=best_graph)
        global_expl.save(global_expl_path)
        logger.info("Saved the explanation at {}".format(global_expl_path))

        return global_expl

    def custom_zipp(self, concepts):
        max_len = max(len(c) for c in concepts)

        if not max_len:
            return []

        for c in concepts:
            if len(c) < max_len:
                c += ["none"] * (max_len - len(c))

        combinations = list(set(itertools.product(*concepts)))

        return combinations

    def get_graph_expl(self, concept_dataset, labels) -> BipartiteGraph:
        """
        Convert a dataset of local concept-based explanations into a bipartite graph
        :param concept_dataset: a dataset of concept-based explanations generated by the Extractor
        :param labels: a list of labels of the task. Each label will correspond to a single partition of the graph.
        :return: A BipartiteGraph object representing a graph loaded with concepts.
        """
        expl_graph = BipartiteGraph(labels)
        label_names = self.extractor.guardian.label_names

        concept_datasets = []
        for i in labels:
            cs = concept_dataset[concept_dataset["GG Label"] == i]
            concept_datasets.append(cs)

        for i, cds in enumerate(concept_datasets):
            if not len(cds):
                continue

            cds["Zipped"] = cds.apply(
                lambda x: self.custom_zipp(
                    [ast.literal_eval(x[label_names[d]]) for d in labels]
                ),
                axis=1,
            )

            concept_pairs = cds["Zipped"].values.tolist()
            flattened_concepts = [item for sublist in concept_pairs for item in sublist]
            expl_graph.load_graph(flattened_concepts, label=i)

        return expl_graph

    def generate_output_folders(self, path, dataset_name):
        """
        Generates two paths for storing local and global explanations generated by the pipeline.
        :param path: Absolute path to a results folder
        :param dataset_name: The name of the dataset being explained
        :return: paths to local and global explanations
        """
        local_expl_path = os.path.join(path, dataset_name, "local")
        global_expl_path = os.path.join(path, dataset_name, "global")

        create_folders = [local_expl_path, global_expl_path]
        for f in create_folders:
            if not os.path.isdir(f):
                os.makedirs(f)

                logger.info("Created results folder at: {}".format(f))

        local_expl_path = os.path.join(local_expl_path, "expl.csv")
        global_expl_path = os.path.join(global_expl_path, "global_expl.pkl")

        return local_expl_path, global_expl_path
